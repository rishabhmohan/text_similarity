{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i4wKXCl2vzc-"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from math import sqrt, pow, exp\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from simple_elmo import ElmoModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(x,y):\n",
        "    \"\"\" Jaccard similarity takes into account only the set of unique words for each text document\n",
        "    if a term like “HD” or “thermal efficiency” is used multiple times in one product description\n",
        "    and just once in another product description, the Euclidean distance and cosine similarity would drop.\n",
        "    On the other hand, if the total number of unique words stays the same, the Jaccard similarity\n",
        "    will remain unchanged. \"\"\"\n",
        "    return len(set.intersection(*[set(x), set(y)]))/ len(set.union(*[set(x), set(y)]))\n"
      ],
      "metadata": {
        "id": "k3c8MmHP5nmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(x,y):\n",
        "    \"\"\" We use spaCy’s in-built Word2Vec model to create text embeddings.\n",
        "    Euclidean distance doesn’t work well with the sparse vectors of text embeddings.\n",
        "    So cosine similarity is generally preferred over Euclidean distance when working with text data\"\"\"\n",
        "    embeddings_x = nlp(x).vector\n",
        "    embeddings_y = nlp(y).vector\n",
        "    distance = sqrt(sum(pow(a-b,2) for a, b in zip(embeddings_x, embeddings_y)))\n",
        "    #we need to normalize distance to the range of 0 to 1\n",
        "    #we can use the Euler’s constant as less sensitive to outliers\n",
        "    return 1/exp(distance)"
      ],
      "metadata": {
        "id": "qvgcvm255wgV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def levenshtein_distance(str1, str2):\n",
        "\n",
        "    '''Aim is to build a 2D matrix and track the cost or changes required\n",
        "       by comparing each both strings character by character.\n",
        "    '''\n",
        "    # Initialize the zero matrix\n",
        "    row_length = len(str1)+1\n",
        "    col_length = len(str2)+1\n",
        "    distance = np.zeros((row_length,col_length),dtype = int)\n",
        "\n",
        "    # Populate matrix of zeros with the indices of each character of both strings\n",
        "    for i in range(1, row_length):\n",
        "        for k in range(1,col_length):\n",
        "            distance[i][0] = i\n",
        "            distance[0][k] = k\n",
        "\n",
        "    # writng loops to find the cost of deletion, addition and substitution\n",
        "    for col in range(1, col_length):\n",
        "        for row in range(1, row_length):\n",
        "            if str1[row-1] == str2[col-1]:\n",
        "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
        "            else:\n",
        "                cost = 1\n",
        "\n",
        "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of removal\n",
        "                                 distance[row][col-1] + 1,          # Cost of addition\n",
        "                                 distance[row-1][col-1] + cost)     # Cost of substitution\n",
        "\n",
        "    return distance[row][col]\n"
      ],
      "metadata": {
        "id": "af6gJH-b54yh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(x,y):\n",
        "    \"\"\" similarity of two vectors as the cosine of the angle between two vectors.\n",
        "    It determines whether two vectors are pointing in roughly the same direction.\n",
        "    So if the angle between the vectors is 0 degrees, then the cosine similarity is 1.\n",
        "    Cosine similarity is not affected by the magnitude/length of the feature vectors. \"\"\"\n",
        "    #spaCy’s in-built Word2Vec model to create text embeddings\n",
        "    embeddings_x = nlp(x).vector\n",
        "    embeddings_y = nlp(y).vector\n",
        "    numerator = sum(a*b for a,b in zip(embeddings_x,embeddings_y))\n",
        "    #return 3 rounded square rooted value\n",
        "    squared_sum = round(sqrt(sum([a*a for a in embeddings])),3)\n",
        "    denominator = squared_sum(embeddings_x)*squared_sum(embeddings_y)\n",
        "    return round(numerator/float(denominator),3)\n"
      ],
      "metadata": {
        "id": "jTfRTxMl5yOx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words_embeddings_similarity(list_of_sentences):\n",
        "     \"\"\"bag of words representation (also called count vectorizing), each word is represented by its count instead of 1.\n",
        "     Regardless of that, both these approaches create huge, sparse vectors that capture absolutely no\n",
        "     relational information\"\"\"\n",
        "     vectorizer = CountVectorizer()\n",
        "     X = vectorizer.fit_transform(list_of_sentences)\n",
        "     return cosine_similarity(X.toarray())\n"
      ],
      "metadata": {
        "id": "JVNnZbLU59s8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_embeddings_similarity(list_of_sentences):\n",
        "    \"\"\"TF-IDF vectors are an extension of the one-hot encoding model. Instead of considering the frequency of words\n",
        "     in one document, the frequency of words across the whole corpus is taken into account. The big idea is that words\n",
        "     that occur a lot everywhere carry very little meaning or significance. Although TF-IDF vectors offer a slight\n",
        "     improvement over simple count vectorizing, they still have very high dimensionality and don’t capture semantic\n",
        "     relationships.\"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(list_of_sentences)\n",
        "    return cosine_similarity(X.toarray())\n"
      ],
      "metadata": {
        "id": "AD4AAp0k5_TU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec_cosine_similarity(list_of_sentences):\n",
        "    \"\"\"CBOW is better at learning syntactic relationships between words while skip-gram is better at understanding\n",
        "    the semantic relationships. In practical terms, this means that for a word like ‘dog’, CBOW would return\n",
        "    morphologically similar words like plurals like 'dogs'. On the other hand, Skip-gram would consider morphologically\n",
        "    different but semantically similar words like 'cat' or 'hamster'\"\"\"\n",
        "    docs = [nlp(sentence) for sentence in list_of_sentences]\n",
        "    similarity = []\n",
        "    for i in range(len(docs)):\n",
        "        row = []\n",
        "        for j in range(len(docs)):\n",
        "            row.append(docs[i].similarity(docs[j]))\n",
        "        similarity.append(row)\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "2pVHv1i46LHh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elmo_embeddings(sentence):\n",
        "    \"\"\"ELMo computes the embeddings from the internal states of a two-layer bidirectional Language Model (LM),\n",
        "     thus the name “ELMo”: Embeddings from Language Models.\"\"\"\n",
        "    model = ElmoModel()\n",
        "    model.load(\"/content/209.zip\")\n",
        "    return model.get_elmo_vectors(sentence, layers=\"average\")\n"
      ],
      "metadata": {
        "id": "Gi8aEAnU6WR9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sbert(sentences):\n",
        "    \"\"\"Sentence-BERT (SBERT) is a modified BERT network that uses siamese and triplet network structures to derive\n",
        "    semantically meaningful sentence embeddings. This reduces the effort for finding the most similar pair from 65\n",
        "    hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT\"\"\"\n",
        "    model = SentenceTransformer('stsb-roberta-large')\n",
        "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    similarity = []\n",
        "    for i in range(len(sentences)):\n",
        "        row = []\n",
        "        for j in range(len(sentences)):\n",
        "          row.append(util.pytorch_cos_sim(embeddings[i], embeddings[j]).item())\n",
        "        similarity.append(row)\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "ppYvrd_L6R5c"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CApa0MOQ6Udb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}